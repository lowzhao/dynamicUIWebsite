<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<!-- <script src="../build/tracking-min.js"></script> -->
	<script src="face-api.js"></script>
	<script src="https://code.jquery.com/jquery-git.min.js"></script>
	<script>
		const MODEL_URL = '/public/models'

		$(document).ready(function () {
			run()
		})

		async function run() {
			// load the models
			await faceapi.loadSsdMobilenetv1Model(MODEL_URL)

			// try to access users webcam and stream the images
			// to the video element
			const videoEl = document.getElementById('inputVideo');
			navigator.mediaDevices.getUserMedia({
				video: {}
			}).then(
				stream => videoEl.srcObject = stream,
				err => console.error(err))


			// const mtcnnForwardParams = {
			// 	// limiting the search space to larger faces for webcam detection
			// 	minFaceSize: 200
			// }


		}

		async function onPlay(videoEl) {
			// run face detection & recognition
			// ...

			const mtcnnForwardParams = {
				// number of scaled versions of the input image passed through the CNN
				// of the first stage, lower numbers will result in lower inference time,
				// but will also be less accurate
				maxNumScales: 10,
				// scale factor used to calculate the scale steps of the image
				// pyramid used in stage 1
				scaleFactor: 0.709,
				// the score threshold values used to filter the bounding
				// boxes of stage 1, 2 and 3
				scoreThresholds: [0.6, 0.7, 0.7],
				// mininum face size to expect, the higher the faster processing will be,
				// but smaller faces won't be detected
				minFaceSize: 200
			}

			const input = document.getElementById('inputVideo')
			detections = await faceapi.detectSingleFace(input)

			// resize the detected boxes in case your displayed image has a different size then the original
			// const detectionsForSize = detections.map(det => det.forSize(input.width, input.height))
			// draw them into a canvas
			const canvas = document.getElementById('overlay')
			canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);

			const box = document.getElementById('hello');
			
			if (detections != undefined) {
				if (250-detections._box._height < 0){
					box.style.fill = "blur(0px)";
				}else{
					box.style.filter ="blur(" +(250-detections._box._height)/10+"px)"
				}
				box.style.boxShadow =  (detections._box.x -300  )/10+ "px " + (150 - detections._box.y)/5 + "px 10px rgb(128, 128, 128)"
				faceapi.drawDetection(canvas, detections, {
					withScore: false
				})
			}
			setTimeout(() => onPlay(videoEl), 30)
		}
	</script>
	<style>
		#hello {
			margin: auto;
			width: 1000px;
			height: 400px;
			box-shadow: 20px 20px 20px rgb(128, 128, 128);
			background-color: white;
			z-index:101;
			top:100px;
			left:20%;
			position: absolute;
			transition: 0.2s;
font-size: 40px;
text-align: center;
		}
		
	</style>
</head>

<body>
	<!-- <video id="myVideo" width="400" height="300" preload autoplay loop muted></video>
  <img src="https://cdn-images-1.medium.com/max/1600/1*rBjbo4EMkQ75zuju54NhZA.jpeg" id="myImage"/> -->
  <!-- <div style="
    top: 0;
    left: 0;
    background-color: white;
    width: 100%;
    position: absolute;
    height: 100%;
    z-index: 100;
"></div> -->
	<div style="position: relative" class="margin">
		<video onplay="onPlay(this)" id="inputVideo" autoplay muted></video>
		<canvas style="position: absolute;
		top: 0;
		left: 0;
	" id="overlay" width="640px" height="480px" />
	</div>
	<div id="hello">
		<div style="padding-top:125px;">
		WELCOME
	</div>
	</div>
	<!-- <script>
  var colors = new tracking.ColorTracker(['magenta', 'cyan', 'yellow']);

  colors.on('track', function(event) {
    if (event.data.length === 0) {
      // No colors were detected in this frame.
    } else {
      event.data.forEach(function(rect) {
        console.log(rect.x, rect.y, rect.height, rect.width, rect.color);
      });
    }
  });

  tracking.track('#myVideo', colors);
  </script> -->
</body>

</html>